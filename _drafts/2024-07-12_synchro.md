---
layout: post
title: Synchronization Barriers
date: 2024-07-12
collection: gpu
---
### Setting the Scene 
&nbsp;&nbsp;&nbsp;&nbsp; I LOVE parallel programming. Short of expensive, and inflexible, custom hardware tailored to parallelize a given algorithm, GPUs are the fastest device you can ask for performing a numerical task in parallel. Numerical tasks abound in the fields of scientific computing, AI, machine learning, data science, and just technology in general. Many of these tasks boil down to solving a system of equations, which can commonly be expressed as a matrix-vector product. Frequently, the matrix representing the "ground truth" system can be decomposed into a sequence of matrix-matrix products. These topics deserve posts of their own, so I will return to speaking about GPUs. 

&nbsp;&nbsp;&nbsp;&nbsp; However powerful they are, GPUs do not operate alone. Not everything that needs to be done are the numerical tasks they are specialialized for. The reason for this powerful specialization, and their need for a partner, is the large number of cores that are located on the device. Individually, each of these cores is weak, typically operating at a rate of just 100s of MHz or into the low GHz, but like apes, together they are strong. However, sometimes, there are tasks which do not benefit from having thousands of cores implementing a SIMD / SIMT model of parallelism. In these situations, for example, if we were trying to manage the runtime of a program which requires a variety of tasks beyond just raw, numerical calculation of vectors, matrices, integrals, and tensors, etc., then we would rather have only a handful of much more powerful cores, operating at much higher clock frequencies, and each of which is adapted to efficiently processing these general instructions. I am of course, talking about a CPU. Modern CPUs are incredible examples of parallel technology in their own right, but they lack the volume to compete with a GPU, and their flexibility and general-purpose instruction processing ability comes at the cost of a non-trivial amount of overhead, when it comes to numerical computation. 

&nbsp;&nbsp;&nbsp;&nbsp; Together, a CPU and a GPU form the minimum relevant configuration for a distributed system (unless you want to be pedantic and refer to a multicore CPU as a distributed system, in which case insert the term *heterogeneous* where appropriate).  

#### Synchronization Barriers - How to Kill Your Speed
&nbsp;&nbsp;&nbsp;&nbsp; All applications are the product of two basic components, the *runtime*, and the *kernels*. The application's runtime is the sequence of things, the kernels, that must be performed for everything to compute, and the kernels are the specific articles of work that must be performed. In a single-threaded, or sequential context, the runtime is straightforward, at least compared to the runtime of a parallel application. First you do A, then you do B, then you do C, then..., you get the picture. To flesh out the runtime you have to write the kernels that **define** what A, B, C, etc., are. Do this, verify it's working, validate it's working as intended, and *voila*, you have working software. 

&nbsp;&nbsp;&nbsp;&nbsp; In a parallel context, we have hardware tools to leverage [concurrency](2024-07-16_concurrency), and do multiple things at once.  