---
layout: post
title: Synchronization Barriers
collection: gpu
---
### Setting the Scene 
&nbsp;&nbsp;&nbsp;&nbsp;I LOVE parallel programming. Short of expensive, and inflexible, custom hardware tailored to parallelize a given algorithm, GPUs are the fastest device you can ask for performing a numerical task in parallel. Numerical tasks abound in the fields of scientific computing, AI, machine learning, data science, and technology in general. However, GPUs do not operate alone; not everything that needs to be done are the numerical tasks they are specialialized for. The reason for this powerful specialization is the large number of cores that are located on the device. 

&nbsp;&nbsp;&nbsp;&nbsp;Individually, each of these cores is weak, typically operating at a rate of just 100s of MHz or into the low GHz, but like apes, together they are strong. However, sometimes, there are tasks which do not benefit from having thousands of cores implementing a SIMD / SIMT model of parallelism. In these situations, for example, if we were trying to manage the runtime of a program which requires a variety of tasks beyond just raw, numerical calculation of vectors, matrices, and tensors, then we would rather have only a handful of much more powerful cores, operating at much higher clock frequencies, and each of which is adapted to efficiently processing these general instructions. I am of course, talking about a CPU. Modern CPUs are incredible examples of parallel technology in their own right, but they lack the volume to compete with a GPU, and their flexibility and general-purpose instruction processing ability comes at the cost of a non-trivial amount of overhead, when it comes to numerical computation. 

&nbsp;&nbsp;&nbsp;&nbsp;Together, a CPU and a GPU form the minimal configuration for a distributed system.  