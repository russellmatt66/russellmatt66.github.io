<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <!-- Enable responsiveness on mobile devices -->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  <title>Strong Scaling</title>
  <link rel="stylesheet" href="/css/main.css">
  <link rel="shortcut icon" type="image/png" href="/assets/killerwhaleph.png">
</head>

  <script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ['\[','\]'] ],
    displayMath: [ ['$$','$$'], ['\\[','\\]'] ],
    processEscapes: true
  },
  TeX: {
    equationNumbers: {autoNumber: "all" },
    extensions: ["AMSmath.js", "AMSsymbols.js"]
  },
  "HTML-CSS": {
    fonts: ["Latin-Modern"],
    mtextFontInherit: true
  }
});

MathJax.Hub.Register.StartupHook("TeX Jax Ready",function() {
  var TEX = MathJax.InputJax.TeX;
  TEX.Definitions.Add({
    macros: {
      ranktwotensor: ["Macro","{\\overline{\\overline{#1}}}",1],
      rowvec: ["Macro","{\\begin{bmatrix} #1 \\end{bmatrix}}", 1],
      colvec: ["Macro","{\\begin{bmatrix} #1 \\end{bmatrix}}", 1],
      quantity:["Macro","{\\left\\{ #1 \\right\\}}",1],
      qty:["Macro","{\\left\\{ #1 \\right\\}}",1],
      pqty:["Macro","{\\left( #1 \\right)}",1],
      bqty:["Macro","{\\left[ #1 \\right]}",1],
      vqty:["Macro","{\\left\\vert #1 \\right\\vert}",1],
      Bqty:["Macro","{\\left\\{ #1 \\right\\}}",1],
      absolutevalue:["Macro","{\\left\\vert #1 \\right\\vert}",1],
      abs:["Macro","{\\left\\vert #1 \\right\\vert}",1],
      norm:["Macro","{\\left\\Vert #1 \\right\\Vert}",1],
      evaluated:["Macro","{#1 \\vert}",1],
      eval:["Macro","{#1 \\vert}",1],
      order:["Macro","{\\mathcal{O} \\left( #1 \\right)}",1],
      commutator:["Macro","{\\left[ #1 , #2 \\right]}",2],
      comm:["Macro","{\\left[ #1 , #2 \\right]}",2],
      anticommutator:["Macro","{\\left\\{ #1 , #2 \\right\\}}",2],
      acomm:["Macro","{\\left\\{ #1 , #2 \\right\\}}",2],
      poissonbracket:["Macro","{\\left\\{ #1 , #2 \\right\\}}",2],
      pb:["Macro","{\\left\\{ #1 , #2 \\right\\}}",2],
      vectorbold:["Macro","{\\boldsymbol{ #1 }}",1],
      vb:["Macro","{\\boldsymbol{ #1 }}",1],
      vectorarrow:["Macro","{\\vec{\\boldsymbol{ #1 }}}",1],
      va:["Macro","{\\vec{\\boldsymbol{ #1 }}}",1],
      vectorunit:["Macro","}}",1],
      vu:["Macro","}}",1],
      dotproduct:["Macro","{\\boldsymbol\\cdot}"],
      vdot:["Macro","{\\boldsymbol\\cdot}"],
      crossproduct:["Macro","{\\boldsymbol\\times}"],
      cross:["Macro","{\\boldsymbol\\times}"],
      cp:["Macro","{\\boldsymbol\\times}"],
      gradient:["Macro","{\\boldsymbol\\nabla}"],
      grad:["Macro","{\\boldsymbol\\nabla}"],
      divergence:["Macro","{\\grad\\vdot}"],
      div:["Macro","{\\grad\\vdot}"],
      curl:["Macro","{\\grad\\cross}"],
      laplacian:["Macro","{\\nabla^2}"],
      tr:["Macro","{\\operatorname{tr}}"],
      Tr:["Macro","{\\operatorname{Tr}}"],
      rank:["Macro","{\\operatorname{rank}}"],
      erf:["Macro","{\\operatorname{erf}}"],
      Res:["Macro","{\\operatorname{Res}}"],
      principalvalue:["Macro","{\\mathcal{P}}"],
      pv:["Macro","{\\mathcal{P}}"],
      PV:["Macro","{\\operatorname{P.V.}}"],
      Re:["Macro","{\\operatorname{Re} \\left\\{ #1 \\right\\}}",1],
      Im:["Macro","{\\operatorname{Im} \\left\\{ #1 \\right\\}}",1],
      qqtext:["Macro","{\\quad\\text{ #1 }\\quad}",1],
      qq:["Macro","{\\quad\\text{ #1 }\\quad}",1],
      qcomma:["Macro","{\\text{,}\\quad}"],
      qc:["Macro","{\\text{,}\\quad}"],
      qcc:["Macro","{\\quad\\text{c.c.}\\quad}"],
      qif:["Macro","{\\quad\\text{if}\\quad}"],
      qthen:["Macro","{\\quad\\text{then}\\quad}"],
      qelse:["Macro","{\\quad\\text{else}\\quad}"],
      qotherwise:["Macro","{\\quad\\text{otherwise}\\quad}"],
      qunless:["Macro","{\\quad\\text{unless}\\quad}"],
      qgiven:["Macro","{\\quad\\text{given}\\quad}"],
      qusing:["Macro","{\\quad\\text{using}\\quad}"],
      qassume:["Macro","{\\quad\\text{assume}\\quad}"],
      qsince:["Macro","{\\quad\\text{since}\\quad}"],
      qlet:["Macro","{\\quad\\text{let}\\quad}"],
      qfor:["Macro","{\\quad\\text{for}\\quad}"],
      qall:["Macro","{\\quad\\text{all}\\quad}"],
      qeven:["Macro","{\\quad\\text{even}\\quad}"],
      qodd:["Macro","{\\quad\\text{odd}\\quad}"],
      qinteger:["Macro","{\\quad\\text{integer}\\quad}"],
      qand:["Macro","{\\quad\\text{and}\\quad}"],
      qor:["Macro","{\\quad\\text{or}\\quad}"],
      qas:["Macro","{\\quad\\text{as}\\quad}"],
      qin:["Macro","{\\quad\\text{in}\\quad}"],
      differential:["Macro","{\\text{d}}"],
      dd:["Macro","{\\text{d}}"],
      derivative:["Macro","{\\frac{\\text{d}{ #1 }}{\\text{d}{ #2 }}}",2],
      dv:["Macro","{\\frac{\\text{d}{ #1 }}{\\text{d}{ #2 }}}",2],
      partialderivative:["Macro","{\\frac{\\partial{ #1 }}{\\partial{ #2 }}}",2],
      <!-- pdv:["Macro","{\\frac{\\partial{ #1 }}{\\partial{ #2 }}}",2], -->
      pdv:["Macro","{\\frac{\\partial^{#2}{ #1 }}{\\partial{ #3^{#2} }}}",3],
      variation:["Macro","{\\delta}"],
      "var":["Macro","{\\delta}"],
      functionalderivative:["Macro","{\\frac{\\delta{ #1 }}{\\delta{ #2 }}}",2],
      fdv:["Macro","{\\frac{\\delta{ #1 }}{\\delta{ #2 }}}",2],
      ket:["Macro","{\\left\\vert { #1 } \\right\\rangle}",1],
      bra:["Macro","{\\left\\langle { #1} \\right\\vert}",1],
      innerproduct:["Macro","{\\left\\langle {#1} \\mid { #2} \\right\\rangle}",2],
      braket:["Macro","{\\left\\langle {#1} \\mid { #2} \\right\\rangle}",2],
      outerproduct:["Macro","{\\left\\vert { #1 } \\right\\rangle\\left\\langle { #2} \\right\\vert}",2],
      dyad:["Macro","{\\left\\vert { #1 } \\right\\rangle\\left\\langle { #2} \\right\\vert}",2],
      ketbra:["Macro","{\\left\\vert { #1 } \\right\\rangle\\left\\langle { #2} \\right\\vert}",2],
      op:["Macro","{\\left\\vert { #1 } \\right\\rangle\\left\\langle { #2} \\right\\vert}",2],
      expectationvalue:["Macro","{\\left\\langle {#1 } \\right\\rangle}",1],
      expval:["Macro","{\\left\\langle {#1 } \\right\\rangle}",1],
      ev:["Macro","{\\left\\langle {#1 } \\right\\rangle}",1],
      matrixelement:["Macro","{\\left\\langle{ #1 }\\right\\vert{ #2 }\\left\\vert{#3}\\right\\rangle}",3],
      matrixel:["Macro","{\\left\\langle{ #1 }\\right\\vert{ #2 }\\left\\vert{#3}\\right\\rangle}",3],
      mel:["Macro","{\\left\\langle{ #1 }\\right\\vert{ #2 }\\left\\vert{#3}\\right\\rangle}",3]
    }
  });
});
</script>

<!-- <script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script> -->
<!-- <script type="text/javascript">
  window.MathJax = {
    tex: {
      extensions: ["AMSmath.js", "AMSsymbols.js"],
      packages: {'[+]': ['mhchem']}
    },
    loader: {
      load: ['[tex]/mhchem']
    }
  };
</script> -->

<script 
  src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_HTMLorMML' async>
</script>

  <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-RT0E6XZ74J"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-RT0E6XZ74J');
</script>

  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="GPU, HPC, fusion energy, and computational science articles">
  <meta name="keywords" content="GPU,HPC,physics,fusion,fusion energy,computational science,c,cuda,python">

  <body>
    <header>
<nav>
  <ul>
    <li class="navhome"><a href="/">Physics In Parallel</a></li>
    
    <li><a href="/phys/">Physics</a>
    
    <li><a href="/math/">Mathematics</a>
    
    <li><a href="/gpu/">GPU</a>
    
    <li><a href="/hpc/">HPC</a>
    
    <li><a href="/compsci/">CS</a>
    
  </ul>
</nav>
</header>

    <div class="container">
      <h1>Strong Scaling</h1>
<p class="meta" style="color:#888">06 Jan 2025</p>
<div class="post">
  <h3 id="death-by-a-thousand-memory-migrations">Death By A Thousand Memory Migrations</h3>
<p>     What kills strong scaling in practice is what also commonly kills distributed performance: <strong>communication overhead</strong>. It is a fact of physics, namely the speed of light, that it will take longer for the information encoded in electrical signals to travel from the memory of Processor A, all the way to the memory of Processor B, than it will for the information to travel from the memory of Processor A to its registers, and back after having been operated on. Information does not just <em>appear</em> in the memories of the Processors that constitute an array of them. It has to begin <em>somewhere</em>, and then be transferred accordingly.</p>

<p>     Let’s consider a very simple problem to solve with parallel programming: we have a list of numbers, and we want to sum them. Let’s make this concrete, and say that we have $N=1024$ numbers. Going a step further, let’s next consider an array of processors. Each processor in the array can either add two numbers together using a single unit of time, or migrate any amount of data to another processor using three units of time. This is actually a very charitable <strong>compute-to-communication ratio</strong> (C2C). For example, in modern GPUs like the <a href="https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf">A100</a> this ratio can be as high as 260x if we are talking about moving FP32 data to a companion device linked to with NVLink instead of processing it with Tensor Cores, and over <strong>2400x</strong> if we need to use PCIe to move this data to a device that is in a different node entirely, or to a host. Even if we were dealing not with Tensor Cores, but just with explicit FP32, we’d still have a 32.5x difference between the speed at which we can compute, and the the speed at which we can transfer data, when utilizing the A100. The modest RTX2060 that I have been primarily developing <a href="https://github.com/russellmatt66/imhd-CUDA">imhd-CUDA</a> with, a CUDA solver for the 3D, time-dependent, Ideal Magnetohydrodynamics system of PDEs, to study fusion equilibria has a C2C of 18.75x. What this means in practice is that wherever possible, you want to <strong>overlap communication with computation</strong>. To implement this, CUDA has the concept of a <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html?highlight=Streams#streams"><em>stream</em></a></p>

<p>     Let’s return to our earlier example.</p>

<!-- References -->
<p>[^1:] Schmidt, B., et al. (2017). <em>Parallel Programming: Concepts and Practice</em> (1st ed.). Morgan Kaufmann.<br />
<a href="https://www.amazon.com/Parallel-Programming-Bertil-Schmidt-Ph-D/dp/0128498900">Amazon</a></p>

</div>

    </div>
    <footer>
  <div>
    <hr>
    <ul>
      <li><a href="mailto:russellmatt66@gmail.com">russellmatt66@gmail.com</a></li>
      <li><a href="https://github.com/russellmatt66">github.com/russellmatt66</a></li>
    </ul>
  </div>
</footer>

  </body>
</html>
