---
layout: post
title: Amdahl's Law
date: 2024-12-17
collection: hpc
---
&nbsp;&nbsp;&nbsp;&nbsp; Every quantitative field has its fundamental laws. Engineers will point to mass, momentum, and energy conservation. Modern physicists will smirk and point to Noether's theorem, instead. Economists will point to supply and demand. Financial professionals will point to compound interest. Mathematicians will point to Godel's Incompleteness Theorem. Computer scientists will gesture in the direction of Alan Turing. In high-performance computing (HPC), the first thing that you learn is how hard it can be to parallelize something. Then, this motivates you to learn how to figure out whether or not you *should*[^1]. 

&nbsp;&nbsp;&nbsp;&nbsp; Every algorithm can be thought of as being a set of sections that are parallelizable, together with a set of sections that are not parallelizable. Pregnancy is the classic example of a process that is not parallelizable. Nine women can have nine babies in nine months, but, as the joke goes, regardless of what the project manager thinks, nine women cannot have one baby in one month. With a single core, the wall-time of the algorithm can be expressed as, 

\begin{align}
T(1) = T_{ser} + T_{par}
\end{align} 

&nbsp;&nbsp;&nbsp;&nbsp; When multiple processors are introduced, which to be sure is not the *only* way to add parallelism, but it is the simplest to think about, then we cut the time of the parallel section accordingly. 
Assuming the workload is evenly balanced, the processors are uniform, and we don't think too much about the rare case when the specific details of the hardware, and problem, align so as to neglect for the moment the situations when *super-linear* speedups occur, then we can assume that $p$ processors will reduce the wall-time of the parallel section by a factor of $p$,

\begin{align}
T(p) \geq T_{ser} + \frac{T_{par}}{p}
\end{align}

&nbsp;&nbsp;&nbsp;&nbsp; In practice, the workload will not be evenly balanced, bottlenecks will exist, etc., so we consider the expression on the RHS to be a lower-bound on the wall-time of the algorithm, hence the sign. What we are *really* interested in is an estimate of the speedup,

$$
S(p) = \frac{T(1)}{T(p)} 
$$

$$
\rightarrow S(p) \leq \frac{T_{ser} + T_{par}}{T_{ser} + \frac{T_{par}}{p}} \label{eqn:speedup}
$$

&nbsp;&nbsp;&nbsp;&nbsp; Frequently, when you have a binary option like this in mathematics, or physics, which adds up to some value, then you know that a coefficient is swiftly going to show up which characterizes how much each of the options contributes. For example, in probability, if there are two outcomes to an event then we can ascribe a probability, $\alpha$, that one of the events occurs. Then, the probability that the other one occurs is $1 - \alpha$. To progress, we do something similar here. We say that there is a coefficient, $\alpha$, which relates how much of the overall runtime is taken by the serial portion,

$$
\begin{align}
T_{ser} &= \alpha T(1) \\
\therefore T_{par} &= (1 - \alpha)T(1) 
\end{align}
$$
 
Consequently, we insert this into Equation (\ref{eqn:speedup}), and rearrange,

$$
\begin{align}
\therefore S(p) &\leq \frac{\alpha T(1) + (1 - \alpha)T(1)}{\alpha T(1) + \frac{1}{p}(1 - \alpha)T(1)} \\
&\leq \frac{\alpha + 1 - \alpha}{\alpha + \frac{1 - \alpha}{p}} \\
&\leq \frac{1}{\alpha + \frac{1 - \alpha}{p}} \label{eqn:amdahl_final}
\end{align}
$$

In general, Equation (\ref{eqn:amdahl_final}) gives us an expression for the speedup. One of the first things that physicists like to do when they find such a thing for what they were looking for is take limits,

$$
\begin{align}
&\lim_{p\rightarrow 1} S(p) \leq 1 \\
&\lim_{p\rightarrow \infty} S(p) \leq \frac{1}{\alpha}
\end{align}
$$

&nbsp;&nbsp;&nbsp;&nbsp; Obviously, the first line is what we expect. When there is only a single core in our cluster, then we expect at best no change in performance. At worst, we might even see a slowdown due to the software overhead from introducing parallelism. The accuracy of this analysis is limited to algorithms which are *strong-scaling*, meaning that even with a fixed problem size more processors will result in a speedup compared to less. As we will see in a moment, this is now always true. More does **not** always equal better, and for a very important reason when it comes to parallel programming. We can already see a hint of this from the second line of the above. Even in the case where we have an infinitude of processing cores, we are still limited by the structure of the algorithm, here represented by the coefficient $\alpha$, regarding what speedup we can obtain.   

### Death By A Thousand Memory Migrations
&nbsp;&nbsp;&nbsp;&nbsp; What kills strong scaling in practice is what also commonly kills distributed performance: **communication overhead**. It is a fact of physics, namely the speed of light, that it will take longer for the information encoded in electrical signals to travel from the memory of Processor A, all the way to the memory of Processor B, than it will for the information to travel from the memory of Processor A to its registers, and back after having been operated on. Information does not just *appear* in the memories of the Processors that constitute an array of them. It has to begin *somewhere*, and then be transferred accordingly. 

&nbsp;&nbsp;&nbsp;&nbsp; Let's consider a very simple problem to solve with parallel programming: we have a list of numbers, and we want to sum them. Let's make this concrete, and say that we have $N=1024$ numbers. Going a step further, let's next consider an array of processors. Each processor in the array can either add a number to their partial sum using a single unit of time, or migrate a number to another processor using three units of time. This is actually a very charitable **compute-to-communication ratio** (C2C). For example, in modern GPUs like the [A100](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf) this ratio can be as high as 260x if we are talking about moving FP32 data to a companion device linked to with NVLink instead of processing it with Tensor Cores, and over **2400x** if we need to use PCIe to move this data to a device that is in a different node entirely. Even if we were dealing not with Tensor Cores, but just with explicit FP32, we'd still have a 32.5x difference between the speed at which we can compute, and the the speed at which we can transfer data, when utilizing the A100. The modest RTX2060 that I have been primarily developing [imhd-CUDA](https://github.com/russellmatt66/imhd-CUDA) with, a CUDA solver for the 3D, time-dependent, Ideal Magnetohydrodynamics system of PDEs, to study fusion equilibria has a C2C of 18.75x.         

<!-- References -->
[^1:] Schmidt, B., et al. (2017). *Parallel Programming: Concepts and Practice* (1st ed.). Morgan Kaufmann.  
[Amazon](https://www.amazon.com/Parallel-Programming-Bertil-Schmidt-Ph-D/dp/0128498900)